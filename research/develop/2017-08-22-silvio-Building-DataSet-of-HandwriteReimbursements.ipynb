{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## According to CEAP: Article 4, paragraph 3 \n",
    "\n",
    "The receipt or invoice must not have erasures, additions or amendments, must be dated and must list without generalizations or abbreviations each of the services or products purchased; it can be:\n",
    "\n",
    "CEAP:\n",
    "3. O documento que comprova o pagamento não pode ter rasura, acréscimos, emendas ou entrelinhas, deve conter data e deve conter os serviços ou materiais descritos item por item, sem generalizações ou abreviaturas, podendo ser:\n",
    "\n",
    "\n",
    "# Sumarizing, the reimbursements can not have (***Generalizations )\n",
    "\n",
    "What i mean by generalization:\n",
    "http://www.camara.gov.br/cota-parlamentar/documentos/publ/2398/2015/5635048.pdf\n",
    "\n",
    "As you can see it doesn't has any description about the consummation \n",
    "\n",
    "And what is not a generalization? \n",
    "\n",
    "http://www.camara.gov.br/cota-parlamentar//documentos/publ/1773/2014/5506259.pdf\n",
    "\n",
    "\n",
    "# Therefore, the goal of this notebook is to build a dataset to train Machine Learn methods to detect generalizations in reimbursements \n",
    "\n",
    "\n",
    "# To this task we built a  gold standard reference containing:\n",
    "\n",
    "1) 1691 reimbursements with generalized descriptions\n",
    "2) 1691 well described (* they are called, positive, negative)\n",
    "\n",
    "# All these reimbursements were validated by hand\n",
    "# Thanks so much everyone involved on it :D\n",
    "\n",
    "https://drive.google.com/file/d/0B6F2XOmMAf28U1FsMTN0QXNPX28/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from tqdm import tqdm\n",
    "from PIL import Image as pil_image\n",
    "from wand.image import Image\n",
    "from serenata_toolbox.datasets import Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets start downloading the reimbursements from the toolbox "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2016-11-19-last-year.xz: 100%|██████████| 6.28M/6.28M [00:11<00:00, 544Kb/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datasets = Datasets('../test/')\n",
    "datasets.downloader.download('2016-11-19-last-year.xz') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Then we read the downloaded reimbursements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the downloaded reimbursements files\n",
    "data = pd.read_csv('../test/2016-11-19-last-year.xz',\n",
    "                   parse_dates=[16],\n",
    "                   dtype={'document_id': np.str,\n",
    "                          'congressperson_id': np.str,\n",
    "                          'congressperson_document': np.str,\n",
    "                          'term_id': np.str,\n",
    "                          'cnpj_cpf': np.str,\n",
    "                          'reimbursement_number': np.str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step build the folder structure \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the Directory structure for our ML model\n",
    "CONST_DIR = '../test/dataset/'\n",
    "directories = [CONST_DIR, CONST_DIR+'training',\n",
    "                        CONST_DIR+'training/positive/',\n",
    "                        CONST_DIR+'training/negative/',\n",
    "                        CONST_DIR+'validation/',\n",
    "                        CONST_DIR+'validation/positive/',\n",
    "                        CONST_DIR+'validation/negative/',\n",
    "                        CONST_DIR+'pos_validation/',\n",
    "                        CONST_DIR+'pos_validation/positive/',\n",
    "                        CONST_DIR+'pos_validation/negative/']\n",
    "\n",
    "for dirs in directories:\n",
    "    if (not os.path.exists(dirs)):\n",
    "        os.mkdir(dirs)\n",
    "        \n",
    "positive = directories[2]\n",
    "negative = directories[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the csv file containing the references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 tocheck  standard\n",
      "index                                                             \n",
      "1      https://jarbas.serenatadeamor.org/#/documentId...         1\n",
      "2      https://jarbas.serenatadeamor.org/#/documentId...         1\n",
      "3      https://jarbas.serenatadeamor.org/#/documentId...         1\n",
      "4      https://jarbas.serenatadeamor.org/#/documentId...         1\n",
      "5      https://jarbas.serenatadeamor.org/#/documentId...         1\n",
      "6      https://jarbas.serenatadeamor.org/#/documentId...         1\n",
      "7      https://jarbas.serenatadeamor.org/#/documentId...         1\n",
      "8      https://jarbas.serenatadeamor.org/#/documentId...         0\n",
      "9      https://jarbas.serenatadeamor.org/#/documentId...         1\n",
      "10     https://jarbas.serenatadeamor.org/#/documentId...         0\n",
      "(3382, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "link = 'https://drive.google.com/uc?export=download&id=0B6F2XOmMAf28OEdBLWVBZ2c1RVk'\n",
    "\n",
    "response = urlopen(link)\n",
    "\n",
    "csv_ref = pd.DataFrame.from_csv(response)\n",
    "print(csv_ref.head(10))\n",
    "print(csv_ref.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the reimbursements from the toolbox to be aligned to our reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57it [00:00, 564.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recupered References: 3382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2910it [00:04, 684.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1438\n",
      "1472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc_ids=[]\n",
    "for index, refs in csv_ref.iterrows():\n",
    "    full_name= refs['tocheck'].split(\"/\")\n",
    "    file_name = full_name[len(full_name)-1]\n",
    "    doc_ids.append(file_name)\n",
    "    \n",
    "print (\"recupered References: {}\".format(len(doc_ids)))    \n",
    "csv_ref['id'] = doc_ids\n",
    "data=data[data['document_id'].isin(doc_ids)]\n",
    "\n",
    "refs=[]\n",
    "for index, item in tqdm(data.iterrows()):\n",
    "        tmp = csv_ref.loc[csv_ref['id'] == item.document_id]['standard']\n",
    "        refs.append(tmp.values[0])\n",
    "data['reference'] = refs\n",
    "print(len(data[data['reference']==1]))\n",
    "print(len(data[data['reference']==0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a direct link to PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Creates a new column 'links' containing an url\n",
    "        for the files in the chamber of deputies website\n",
    "        Return updated Dataframe\n",
    "        arguments:\n",
    "        record -- Dataframe\n",
    "\"\"\"       \n",
    "def __document_url(X):\n",
    "    X['link'] = ''\n",
    "    links = list()\n",
    "    for index, x in X.iterrows():\n",
    "        base = \"http://www.camara.gov.br/cota-parlamentar/documentos/publ\"\n",
    "        url = '{}/{}/{}/{}.pdf'.format(base, x.applicant_id, x.year, x.document_id)\n",
    "        links.append(url)\n",
    "    X['link'] = links\n",
    "    return X\n",
    "\n",
    "\n",
    "data = __document_url(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the PDFs and convert them to PNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Case you DO NOT WANT to download all dataset set STOP_AFTER bigger than 0 and lower than 1600. \n",
    "# It will download the same amount for positive and negative samples\n",
    "# Case you WANT all put 0\n",
    "STOP_AFTER = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE:  (1438, 31)\n",
      "NEGATIVE:  (1472, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [02:26,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during pdf download {} http://www.camara.gov.br/cota-parlamentar/documentos/publ/1454/2015/5919642.pdf\n",
      "HTTP Error 404: Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [03:45,  3.86s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Download a pdf file and transform it to png\n",
    "        arguments:\n",
    "        url -- the url to chamber of deputies web site, e.g.,\n",
    "        http://www.../documentos/publ/2437/2015/5645177.pdf\n",
    "        file_name -- myDirectory/5645177.png\n",
    "        Exception -- returns None\n",
    "\"\"\"\n",
    "def download_doc(url_link, file_name):\n",
    "    try:\n",
    "        # Open the resquest and get the file\n",
    "        response = urlopen(url_link)\n",
    "        if (response is not None):\n",
    "            # Default arguments to read the file and has a good resolution\n",
    "            with Image(file=response, resolution=300) as img:\n",
    "                img.compression_quality = 99\n",
    "                # Chosen format to convert pdf to image\n",
    "                with img.convert('png') as converted:\n",
    "                        converted.save(filename=file_name)\n",
    "                        return True\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as ex:\n",
    "            print(\"Error during pdf download {}\",url_link)\n",
    "            print(ex)\n",
    "            # Case we get some exception we return None\n",
    "            return None\n",
    "        \n",
    "def download():\n",
    "    for index, item in tqdm(data.iterrows()):\n",
    "        file_name = item.document_id+'.png'\n",
    "        if(item.reference == 1):\n",
    "            file_name = os.path.join(positive, file_name)\n",
    "            request = download_doc(item.link, file_name)\n",
    "        else:\n",
    "            file_name = os.path.join(negative, file_name)\n",
    "            request = download_doc(item.link, file_name)\n",
    "    \n",
    "if(STOP_AFTER != 0 and STOP_AFTER<1690): \n",
    "    tmp_pos = data[data['reference']==1]\n",
    "    print(\"POSITIVE: \",tmp_pos.shape)\n",
    "    tmp_pos = tmp_pos.sample(STOP_AFTER)\n",
    "    tmp_neg = data[data['reference']==0]\n",
    "    print(\"NEGATIVE: \",tmp_neg.shape)\n",
    "    tmp_neg = tmp_neg.sample(STOP_AFTER)\n",
    "    \n",
    "    data = pd.concat([tmp_pos,tmp_neg])\n",
    "    download()\n",
    "else:\n",
    "    download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split our data in\n",
    "\n",
    "### 70 % of reimbursements for training\n",
    "### 15 % for validation\n",
    "### 15 % for pos validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(len_samples,directory_src,directory_dest):\n",
    "    for x in range(1,len_samples):\n",
    "        current_files = glob.glob(directory_src+'*.png')\n",
    "        file_name = re.sub(directory_src, r'', current_files[0])\n",
    "        shutil.move(os.path.join(directory_src, file_name),  os.path.join(directory_dest, file_name))\n",
    "\n",
    "# Split our Files in Training, Validation\n",
    "# 70% tranning and 30% validation\n",
    "len_val_positive = int(len(glob.glob(positive+'*.png'))*0.3)\n",
    "len_val_negative = int(len(glob.glob(negative+'*.png'))*0.3)\n",
    "\n",
    "split_data(len_val_positive,positive,directories[5])\n",
    "split_data(len_val_negative,negative,directories[6])\n",
    "\n",
    "# Split the Validation in 2 for POS validation\n",
    "len_val_positive = int(len(glob.glob(directories[5]+'*.png'))/2)\n",
    "len_val_negative = int(len(glob.glob(directories[6]+'*.png'))/2)\n",
    "\n",
    "split_data(len_val_positive,directories[5],directories[8])\n",
    "split_data(len_val_negative,directories[6],directories[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets verify how much data we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of trained samples =  53\n",
      " no. of validation samples=  12\n",
      " no. of validation samples=  7\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = CONST_DIR+'training'\n",
    "validation_data_dir =  CONST_DIR+'validation'\n",
    "pos_validation_data_dir = CONST_DIR+'pos_validation'\n",
    "\n",
    "\n",
    "nb_train_samples = sum([len(files) for r, d, files in os.walk(train_data_dir)])\n",
    "nb_validation_samples = sum([len(files) for r, d, files in os.walk(validation_data_dir)])\n",
    "nb_pos_validation_samples = sum([len(files) for r, d, files in os.walk(pos_validation_data_dir)])\n",
    "\n",
    "print('no. of trained samples = ', nb_train_samples)\n",
    "print('no. of validation samples= ',nb_validation_samples)\n",
    "print('no. of pos validation samples= ',nb_pos_validation_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
