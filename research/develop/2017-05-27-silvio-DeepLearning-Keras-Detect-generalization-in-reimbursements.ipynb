{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building powerful image classification models using very little data\n",
    "\n",
    "This notebook was based in this link:\n",
    "https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "\n",
    "They have good explanation and good images to show how these networks compute image classification.\n",
    "So, before to continue go there!\n",
    "Ps: I only commented in my code the strong changes regarding they example.\n",
    "\n",
    "To use it i'm supposing you have installed the requirements to convert pdf to images.\n",
    "\n",
    "## Togheter with these previous requirements you have to install  Keras 2.0 API\n",
    "\n",
    "## Keras: Deep Learning library for TensorFlow and Theano\n",
    "https://github.com/fchollet/keras\n",
    "\n",
    "\n",
    "# Main constraint of this approach: We need a training and validation set :/ \n",
    "\n",
    "## Solution >>> Let's build it.\n",
    "\n",
    "\n",
    "#### It is composed by 1691 wrong reimbursements, and 1691 not wrong (* they are called, positive, negative)\n",
    "\n",
    "What i mean by wrong: http://www.camara.gov.br/cota-parlamentar/documentos/publ/2398/2015/5635048.pdf\n",
    "\n",
    "As you can see it don't has any description about the consummation \n",
    "\n",
    "And what is \"NOT WRONG\": \n",
    "\n",
    "http://www.camara.gov.br/cota-parlamentar//documentos/publ/1773/2014/5506259.pdf\n",
    "\n",
    "# All these reimbursements were validated by hand\n",
    "# Thanks so much everyone involved on it :D\n",
    "\n",
    "Take a look at this great collaborative work: https://docs.google.com/spreadsheets/d/1o7P79iMw2VnJypSZNHrsDjud398g4vXpZdrGMMqe6qA/edit?usp=sharing\n",
    "\n",
    "Here: you can find the up-to-date reimbursements\n",
    "https://drive.google.com/file/d/0B6F2XOmMAf28U1FsMTN0QXNPX28/view?usp=sharing\n",
    "\n",
    "\n",
    "## PS: The first training set was also reevaluated after discussion with @anaschwendler\n",
    "### In the spreadsheet they are in orange color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2016-11-19-last-year.xz: 100%|██████████| 6.28M/6.28M [00:18<00:00, 348Kb/s]\n"
     ]
    }
   ],
   "source": [
    "# First download the dataset\n",
    "from serenata_toolbox.datasets import Datasets\n",
    "datasets = Datasets('../test/')\n",
    "datasets.downloader.download('2016-11-19-last-year.xz') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 tocheck  standard\n",
      "index                                                             \n",
      "1      https://jarbas.serenatadeamor.org/#/documentId...         1\n",
      "2      https://jarbas.serenatadeamor.org/#/documentId...         1\n",
      "3      https://jarbas.serenatadeamor.org/#/documentId...         1\n",
      "4      https://jarbas.serenatadeamor.org/#/documentId...         1\n",
      "5      https://jarbas.serenatadeamor.org/#/documentId...         1\n",
      "6      https://jarbas.serenatadeamor.org/#/documentId...         1\n",
      "7      https://jarbas.serenatadeamor.org/#/documentId...         1\n",
      "8      https://jarbas.serenatadeamor.org/#/documentId...         0\n",
      "9      https://jarbas.serenatadeamor.org/#/documentId...         1\n",
      "10     https://jarbas.serenatadeamor.org/#/documentId...         0\n",
      "(3382, 2)\n",
      "recupered References: 3382\n",
      "0 1 10\n",
      "0 2 10\n",
      "0 3 10\n",
      "1 3 10\n",
      "2 3 10\n",
      "2 4 10\n",
      "2 5 10\n",
      "3 5 10\n",
      "4 5 10\n",
      "4 6 10\n",
      "4 7 10\n",
      "5 7 10\n",
      "5 8 10\n",
      "5 9 10\n",
      "5 10 10\n",
      "5 11 10\n",
      "5 12 10\n",
      "5 13 10\n",
      "5 14 10\n",
      "5 15 10\n",
      "6 15 10\n",
      "7 15 10\n",
      "8 15 10\n",
      "9 15 10\n",
      "10 15 10\n",
      "11 15 10\n",
      "../test/dataset/training/positive/\n",
      "../test/dataset/validation/positive/\n",
      "['../test/dataset/training/positive/5620391.png', '../test/dataset/training/positive/5635671.png', '../test/dataset/training/positive/5635672.png', '../test/dataset/training/positive/5680018.png', '../test/dataset/training/positive/5689464.png', '../test/dataset/training/positive/5689632.png', '../test/dataset/training/positive/5800206.png', '../test/dataset/training/positive/5826294.png', '../test/dataset/training/positive/5832539.png', '../test/dataset/training/positive/5837914.png', '../test/dataset/training/positive/5859794.png']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f7cd949b922c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0mlen_val_negative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'*.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_val_positive\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirectories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_val_negative\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirectories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-f7cd949b922c>\u001b[0m in \u001b[0;36msplit_data\u001b[0;34m(len_samples, directory_src, directory_dest)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mcurrent_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_dest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "import re\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image as pil_image\n",
    "from wand.image import Image\n",
    "\n",
    "\"\"\"Download a pdf file and transform it to png\n",
    "        arguments:\n",
    "        url -- the url to chamber of deputies web site, e.g.,\n",
    "        http://www.../documentos/publ/2437/2015/5645177.pdf\n",
    "        file_name -- myDirectory/5645177.png\n",
    "        Exception -- returns None\n",
    "\"\"\"\n",
    "def download_doc(url_link, file_name):\n",
    "    try:\n",
    "        # Open the resquest and get the file\n",
    "        response = urlopen(url_link)\n",
    "        if (response is not None):\n",
    "            # Default arguments to read the file and has a good resolution\n",
    "            with Image(file=response, resolution=300) as img:\n",
    "                img.compression_quality = 99\n",
    "                # Chosen format to convert pdf to image\n",
    "                with img.convert('png') as converted:\n",
    "                        converted.save(filename=file_name)\n",
    "                        return True\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as ex:\n",
    "            print(\"Error during pdf download {}\",url_link)\n",
    "            print(ex)\n",
    "            # Case we get some exception we return None\n",
    "            return None\n",
    "        \n",
    "\"\"\" Creates a new column 'links' containing an url\n",
    "        for the files in the chamber of deputies website\n",
    "        Return updated Dataframe\n",
    "        arguments:\n",
    "        record -- Dataframe\n",
    "\"\"\"       \n",
    "def __document_url(X):\n",
    "    X['link'] = ''\n",
    "    links = list()\n",
    "    for index, x in X.iterrows():\n",
    "        base = \"http://www.camara.gov.br/cota-parlamentar/documentos/publ\"\n",
    "        url = '{}/{}/{}/{}.pdf'.format(base, x.applicant_id, x.year, x.document_id)\n",
    "        links.append(url)\n",
    "    X['link'] = links\n",
    "    return X\n",
    "\n",
    "# Reading the downloaded reimbursements files\n",
    "data = pd.read_csv('../test/2016-11-19-last-year.xz',\n",
    "                   parse_dates=[16],\n",
    "                   dtype={'document_id': np.str,\n",
    "                          'congressperson_id': np.str,\n",
    "                          'congressperson_document': np.str,\n",
    "                          'term_id': np.str,\n",
    "                          'cnpj_cpf': np.str,\n",
    "                          'reimbursement_number': np.str})\n",
    "\n",
    "# Build the Directory structure for our ML model\n",
    "CONST_DIR = '../test/dataset/'\n",
    "directories = [CONST_DIR, CONST_DIR+'training',\n",
    "                        CONST_DIR+'training/positive/',\n",
    "                        CONST_DIR+'training/negative/',\n",
    "                        CONST_DIR+'validation/',\n",
    "                        CONST_DIR+'validation/positive/',\n",
    "                        CONST_DIR+'validation/negative/',\n",
    "                        CONST_DIR+'pos_validation/',\n",
    "                        CONST_DIR+'pos_validation/positive/',\n",
    "                        CONST_DIR+'pos_validation/negative/',\n",
    "                        CONST_DIR+'save_model/']\n",
    "\n",
    "for dirs in directories:\n",
    "    if (not os.path.exists(dirs)):\n",
    "        os.mkdir(dirs)\n",
    "\n",
    "positive = directories[2]\n",
    "negative = directories[3]\n",
    "\n",
    "#I will look only the meals\n",
    "data=data[data['subquota_description']=='Congressperson meal']\n",
    "\n",
    "# Reference for our model.\n",
    "link = 'https://drive.google.com/uc?export=download&id=0B6F2XOmMAf28OEdBLWVBZ2c1RVk'\n",
    "\n",
    "# Case you DO NOT WANT to download all dataset put some value bigger than 0\n",
    "# Case you WANT all put 0\n",
    "STOP_AFTER = 10\n",
    "\n",
    "response = urlopen(link)\n",
    "\n",
    "csv_ref = pd.DataFrame.from_csv(response)\n",
    "print(csv_ref.head(10))\n",
    "print(csv_ref.shape)\n",
    "doc_ids=[]\n",
    "\n",
    "for index, refs in csv_ref.iterrows():\n",
    "    full_name= refs['tocheck'].split(\"/\")\n",
    "    file_name = full_name[len(full_name)-1]\n",
    "    doc_ids.append(file_name)\n",
    "    \n",
    "print (\"recupered References: {}\".format(len(doc_ids)))    \n",
    "\n",
    "data=data[data['document_id'].isin(doc_ids)]\n",
    "data['reference'] = csv_ref['standard']\n",
    "data = __document_url(data)\n",
    "\n",
    "pos_downloaded = 0\n",
    "neg_downloaded = 0\n",
    "for index, item in data.iterrows():\n",
    "    file_name = item.document_id+'.png'\n",
    "    if(item.reference == 1 and pos_downloaded <= STOP_AFTER):\n",
    "        file_name = os.path.join(positive, file_name)\n",
    "        request = download_doc(item.link, file_name)\n",
    "    elif(neg_downloaded <= STOP_AFTER):\n",
    "        file_name = os.path.join(negative, file_name)\n",
    "        request = download_doc(item.link, file_name)\n",
    "        \n",
    "    if(request is None):\n",
    "        print(\"Error while downloading reimbursement: \",item.link)\n",
    "    elif(item.reference == 1):\n",
    "        # Counting the references\n",
    "        pos_downloaded += 1\n",
    "    else:\n",
    "        neg_downloaded += 1\n",
    "    # Stop after to donwload all informed quantity\n",
    "    print(neg_downloaded,pos_downloaded,STOP_AFTER)\n",
    "    if(STOP_AFTER!=0 and neg_downloaded>STOP_AFTER and pos_downloaded>STOP_AFTER):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../test/dataset/training/positive/\n",
      "../test/dataset/validation/positive/\n",
      "../test/dataset/training/positive/5620391.png\n",
      "../test/dataset/training/positive/\n",
      "../test/dataset/validation/positive/\n",
      "../test/dataset/training/positive/5635671.png\n",
      "../test/dataset/training/negative/\n",
      "../test/dataset/validation/negative/\n",
      "../test/dataset/training/negative/5609327.png\n",
      "../test/dataset/training/negative/\n",
      "../test/dataset/validation/negative/\n",
      "../test/dataset/training/negative/5610943.png\n",
      "../test/dataset/training/negative/\n",
      "../test/dataset/validation/negative/\n",
      "../test/dataset/training/negative/5639417.png\n"
     ]
    }
   ],
   "source": [
    "def split_data(len_samples,directory_src,directory_dest):\n",
    "    for x in range(1,len_samples):\n",
    "        current_files = glob.glob(directory_src+'*.png')\n",
    "        print(directory_src)\n",
    "        print(directory_dest)\n",
    "        print(current_files[0])\n",
    "        file_name = re.sub(directory_src, r'', current_files[0])\n",
    "        shutil.move(os.path.join(directory_src, file_name),  os.path.join(directory_dest, file_name))\n",
    "\n",
    "# Split our Files in Training, Validation\n",
    "# 70% tranning and 30% validation\n",
    "len_val_positive = int(len(glob.glob(positive+'*.png'))*0.3)\n",
    "len_val_negative = int(len(glob.glob(negative+'*.png'))*0.3)\n",
    "\n",
    "split_data(len_val_positive,positive,directories[5])\n",
    "split_data(len_val_negative,negative,directories[6])\n",
    "\n",
    "# Split the Validation in 2 for POS validation\n",
    "len_val_positive = int(len(glob.glob(directories[5]+'*.png'))*0.5)\n",
    "len_val_negative = int(len(glob.glob(directories[6]+'*.png'))*0.5)\n",
    "\n",
    "split_data(len_val_positive,directories[5],directories[8])\n",
    "split_data(len_val_negative,directories[6],directories[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of trained samples =  21  no. of validation samples=  5\n",
      "Found 21 images belonging to 2 classes.\n",
      "Found 5 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      " 5/10 [==============>...............] - ETA: 26s - loss: 8.4402 - acc: 0.3000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-51c76567c7c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     validation_steps=nb_validation_samples // batch_size)\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1108\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1888\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1889\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1631\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1633\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1634\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2229\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os.path\n",
    "import numpy as np\n",
    "\n",
    "#fix random seed for reproducibility\n",
    "seed = 2017\n",
    "np.random.seed(seed)\n",
    "\n",
    "train_data_dir = '../test/dataset/training/'\n",
    "validation_data_dir = '../test/dataset/validation/'\n",
    "\n",
    "\n",
    "\n",
    "nb_train_samples = sum([len(files) for r, d, files in os.walk(train_data_dir)])\n",
    "nb_validation_samples = sum([len(files) for r, d, files in os.walk(validation_data_dir)])\n",
    "\n",
    "print('no. of trained samples = ', nb_train_samples, ' no. of validation samples= ',nb_validation_samples)\n",
    "\n",
    "\n",
    "#dimensions of our images.\n",
    "img_width, img_height = 800, 600\n",
    "\n",
    "\n",
    "epochs = 10 \n",
    "batch_size = 2\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=False)#As you can see i put it as FALSE and on link example it is TRUE\n",
    "#Explanation, there no possibility to write in a reverse way :P\n",
    "\n",
    "#this is the augmentation configuration we will use for testing:\n",
    "#only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "#It allow us to save only the best model between the iterations \n",
    "checkpointer = ModelCheckpoint(filepath=os.path.join(directories[10],\"weights.hdf5\"), verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "     callbacks=[checkpointer], #And we set the parameter to save only the best model\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result: A network with 94% of accuracy!!! Big improvement regarding the first we buit...\n",
    "\n",
    "156/157 [============================>.] - ETA: 3s - loss: 0.3726 - acc: 0.8682 Epoch 00013: val_loss improved from 0.23616 to 0.22647, saving model to weights.hdf5\n",
    "157/157 [==============================] - 607s - loss: 0.3715 - acc: 0.8691 - val_loss: 0.2265 - val_acc: 0.9423\n",
    "\n",
    "# Let's use it on an external set of reimbursements!\n",
    "### @vmesel recommended it, thanks for the feedback :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def goldStandard(png_directory,value):\n",
    "    png = glob.glob(png_directory+'*.png')\n",
    "    data = list()\n",
    "    for f in png:\n",
    "        data.append(f)\n",
    "    df = pd.DataFrame(data,columns=['Image'])\n",
    "    df['Reference']=value\n",
    "   \n",
    "    return df\n",
    "\n",
    "png_directory='../test/dataset/pos_validation/positive/'\n",
    "df1 = goldStandard(png_directory,1)\n",
    "png_directory='../test/dataset/pos_validation/negative/'\n",
    "df2= goldStandard(png_directory,0)\n",
    "frames = [df1, df2]\n",
    "df = pd.concat(frames)\n",
    "print(df.head())\n",
    "print(df.tail())\n",
    "test_model = load_model(filepath=os.path.join(directories[10],\"weights.hdf5\"))#I'm using the saved file to load the model\n",
    "\n",
    "#dimensions of our images.\n",
    "img_width, img_height = 300, 300\n",
    "predicted=list()\n",
    "for obj in df.iterrows():\n",
    "    try:\n",
    "        print(obj[1].Image)\n",
    "        img = load_img(obj[1].Image,False,target_size=(img_width,img_height))#read a image\n",
    "        x = img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0) #convert it\n",
    "        preds = test_model.predict_classes(x) #predict it in our model :D\n",
    "        prob = test_model.predict_proba(x) #get the probability of prediciton\n",
    "        if(prob>=0.8 and preds==1):#Only keep the predictions with more than 80% of accuracy and the class 1 (suspicious)\n",
    "            print(\"suspicious!!! prob:\",prob)\n",
    "            predicted.append(1)\n",
    "        else:\n",
    "            predicted.append(0)\n",
    "    except Exception as ex:\n",
    "            print(ex)\n",
    "df['Predicted']=predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After to run the Model over the pos_validation set\n",
    "## Let's verify how is the performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, _= metrics.roc_curve(df.Reference,df.Predicted)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"Confusion matrix\")\n",
    "print(metrics.confusion_matrix(df.Reference,df.Predicted))\n",
    "print(\" accuracy \",metrics.accuracy_score(df.Reference,df.Predicted))\n",
    "print(\" AUC \",roc_auc)\n",
    "print(\" precision \",metrics.precision_score(df.Reference,df.Predicted))\n",
    "print(\" recall \",metrics.recall_score(df.Reference,df.Predicted))\n",
    "print(\" f1-score \",metrics.f1_score(df.Reference,df.Predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# These results are amazing!! All metrics are above 91% !!\n",
    "\n",
    "# Conclusion:\n",
    "## We have a new classifier which detects generalization in the reimbursements\n",
    "\n",
    "## It handle with CEAP: Article 4, paragraph 3 (***Generalizations )\n",
    "The receipt or invoice must not have erasures, additions or amendments, must be dated and must list without generalizations or abbreviations each of the services or products purchased; it can be:\n",
    "\n",
    "CEAP:\n",
    "3. O documento que comprova o pagamento não pode ter rasura, acréscimos, emendas ou entrelinhas, deve conter data e deve conter os serviços ou materiais descritos item por item, sem generalizações ou abreviaturas, podendo ser:\n",
    "\n",
    "\n",
    "# How to use it?\n",
    "\n",
    "### See this PULL Request : https://github.com/datasciencebr/rosie/pull/66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PS: I would like to discuss some data in the train set\n",
    "\n",
    "In the folder: \"not wrong\", the recipe: 5496084.pdf \n",
    "\n",
    "It is clear to me that the description of the items was made by someone else than the restaurant, is it allowed ???\n",
    "\n",
    "Are the deputies or assessors changing a document?? What are the implications about it?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
